{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Trabalho Prático 2\n",
    "\n",
    "##### **Nome:** André Luiz Moreira Dutra\n",
    "##### **Matrícula:** 2019006345"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Neste trabalho, foi implementado um modelo baseado em BERT para a tarefa de POS-tagging. A tarefa de POS-tagging consiste em, dada uma frase, classificar cada palavra segundo sua classe gramatical na língua. O modelo implementado neste realiza a tarefa de POS-tagging para a língua portuguesa.\n",
    "\n",
    "Primeiramente, vamos instalar e importar todos os pacotes necessários para este trabalho:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "wYJa3mfwPrda",
    "outputId": "fe71b382-23d4-488f-d055-bc1fad9a0c50"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: datasets in /home/andre/miniconda3/envs/notebook_env/lib/python3.10/site-packages (2.12.0)\n",
      "Requirement already satisfied: evaluate in /home/andre/miniconda3/envs/notebook_env/lib/python3.10/site-packages (0.4.1)\n",
      "Requirement already satisfied: seqeval in /home/andre/miniconda3/envs/notebook_env/lib/python3.10/site-packages (1.2.2)\n",
      "Requirement already satisfied: transformers[torch] in /home/andre/miniconda3/envs/notebook_env/lib/python3.10/site-packages (4.29.2)\n",
      "Requirement already satisfied: filelock in /home/andre/miniconda3/envs/notebook_env/lib/python3.10/site-packages (from transformers[torch]) (3.9.0)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.14.1 in /home/andre/miniconda3/envs/notebook_env/lib/python3.10/site-packages (from transformers[torch]) (0.15.1)\n",
      "Requirement already satisfied: numpy>=1.17 in /home/andre/miniconda3/envs/notebook_env/lib/python3.10/site-packages (from transformers[torch]) (1.24.3)\n",
      "Requirement already satisfied: packaging>=20.0 in /home/andre/.local/lib/python3.10/site-packages (from transformers[torch]) (23.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /home/andre/.local/lib/python3.10/site-packages (from transformers[torch]) (6.0.1)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /home/andre/miniconda3/envs/notebook_env/lib/python3.10/site-packages (from transformers[torch]) (2022.7.9)\n",
      "Requirement already satisfied: requests in /home/andre/miniconda3/envs/notebook_env/lib/python3.10/site-packages (from transformers[torch]) (2.31.0)\n",
      "Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in /home/andre/miniconda3/envs/notebook_env/lib/python3.10/site-packages (from transformers[torch]) (0.13.2)\n",
      "Requirement already satisfied: tqdm>=4.27 in /home/andre/miniconda3/envs/notebook_env/lib/python3.10/site-packages (from transformers[torch]) (4.65.0)\n",
      "Requirement already satisfied: torch!=1.12.0,>=1.9 in /home/andre/.local/lib/python3.10/site-packages (from transformers[torch]) (1.12.1)\n",
      "Requirement already satisfied: accelerate>=0.19.0 in /home/andre/miniconda3/envs/notebook_env/lib/python3.10/site-packages (from transformers[torch]) (0.25.0)\n",
      "Requirement already satisfied: pyarrow>=8.0.0 in /home/andre/miniconda3/envs/notebook_env/lib/python3.10/site-packages (from datasets) (11.0.0)\n",
      "Requirement already satisfied: dill<0.3.7,>=0.3.0 in /home/andre/miniconda3/envs/notebook_env/lib/python3.10/site-packages (from datasets) (0.3.6)\n",
      "Requirement already satisfied: pandas in /home/andre/.local/lib/python3.10/site-packages (from datasets) (2.0.1)\n",
      "Requirement already satisfied: xxhash in /home/andre/miniconda3/envs/notebook_env/lib/python3.10/site-packages (from datasets) (2.0.2)\n",
      "Requirement already satisfied: multiprocess in /home/andre/miniconda3/envs/notebook_env/lib/python3.10/site-packages (from datasets) (0.70.14)\n",
      "Requirement already satisfied: fsspec>=2021.11.1 in /home/andre/miniconda3/envs/notebook_env/lib/python3.10/site-packages (from fsspec[http]>=2021.11.1->datasets) (2023.4.0)\n",
      "Requirement already satisfied: aiohttp in /home/andre/miniconda3/envs/notebook_env/lib/python3.10/site-packages (from datasets) (3.8.3)\n",
      "Requirement already satisfied: responses<0.19 in /home/andre/miniconda3/envs/notebook_env/lib/python3.10/site-packages (from datasets) (0.13.3)\n",
      "Requirement already satisfied: scikit-learn>=0.21.3 in /home/andre/.local/lib/python3.10/site-packages (from seqeval) (1.2.2)\n",
      "Requirement already satisfied: psutil in /home/andre/miniconda3/envs/notebook_env/lib/python3.10/site-packages (from accelerate>=0.19.0->transformers[torch]) (5.9.0)\n",
      "Requirement already satisfied: safetensors>=0.3.1 in /home/andre/miniconda3/envs/notebook_env/lib/python3.10/site-packages (from accelerate>=0.19.0->transformers[torch]) (0.4.1)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /home/andre/miniconda3/envs/notebook_env/lib/python3.10/site-packages (from aiohttp->datasets) (22.1.0)\n",
      "Requirement already satisfied: charset-normalizer<3.0,>=2.0 in /home/andre/miniconda3/envs/notebook_env/lib/python3.10/site-packages (from aiohttp->datasets) (2.0.4)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /home/andre/miniconda3/envs/notebook_env/lib/python3.10/site-packages (from aiohttp->datasets) (6.0.2)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /home/andre/miniconda3/envs/notebook_env/lib/python3.10/site-packages (from aiohttp->datasets) (4.0.2)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /home/andre/miniconda3/envs/notebook_env/lib/python3.10/site-packages (from aiohttp->datasets) (1.8.1)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /home/andre/miniconda3/envs/notebook_env/lib/python3.10/site-packages (from aiohttp->datasets) (1.3.3)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /home/andre/miniconda3/envs/notebook_env/lib/python3.10/site-packages (from aiohttp->datasets) (1.2.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /home/andre/.local/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.14.1->transformers[torch]) (4.6.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/andre/miniconda3/envs/notebook_env/lib/python3.10/site-packages (from requests->transformers[torch]) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/andre/miniconda3/envs/notebook_env/lib/python3.10/site-packages (from requests->transformers[torch]) (1.26.16)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/andre/miniconda3/envs/notebook_env/lib/python3.10/site-packages (from requests->transformers[torch]) (2023.7.22)\n",
      "Requirement already satisfied: six in /home/andre/miniconda3/envs/notebook_env/lib/python3.10/site-packages (from responses<0.19->datasets) (1.16.0)\n",
      "Requirement already satisfied: scipy>=1.3.2 in /home/andre/.local/lib/python3.10/site-packages (from scikit-learn>=0.21.3->seqeval) (1.10.1)\n",
      "Requirement already satisfied: joblib>=1.1.1 in /home/andre/.local/lib/python3.10/site-packages (from scikit-learn>=0.21.3->seqeval) (1.2.0)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /home/andre/.local/lib/python3.10/site-packages (from scikit-learn>=0.21.3->seqeval) (3.1.0)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /home/andre/.local/lib/python3.10/site-packages (from pandas->datasets) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /home/andre/miniconda3/envs/notebook_env/lib/python3.10/site-packages (from pandas->datasets) (2022.7)\n",
      "Requirement already satisfied: tzdata>=2022.1 in /home/andre/.local/lib/python3.10/site-packages (from pandas->datasets) (2023.3)\n"
     ]
    }
   ],
   "source": [
    "#Instalação dos pacotes necessários\n",
    "! pip install transformers[torch] datasets evaluate seqeval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "YvUCYAb5BARk"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-12-13 15:03:14.599823: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "from transformers import AutoTokenizer\n",
    "from transformers import DataCollatorForTokenClassification\n",
    "\n",
    "import evaluate\n",
    "from numpy import argmax\n",
    "\n",
    "import torch\n",
    "from transformers import AutoModelForTokenClassification\n",
    "from transformers import Trainer, TrainingArguments\n",
    "\n",
    "from torch.utils.data import DataLoader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset Utilizado\n",
    "\n",
    "O dataset utilizado é o Mac-Morpho, um dataset projetado especificamente para a tarefa de POS-tagging para a língua portuguesa. O dataset consiste em cerca de 50.000 entradas, com cada uma contendo uma sentença tokenizada e a respectiva lista de POS-tags de cada token da sentença. O dataset é dividido em 76%-4%-20% para treino, validação e teste. Cada token é classificado em uma de 26 POS-tags diferentes.\n",
    "\n",
    "Neste trabalho, o Mac-Morpho foi carregado diretamente do huggingface conforme o seguinte código:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "-6pgmk42twPg",
    "outputId": "9a71be7d-5c57-40e7-fe1c-e14a52ab44fa"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading and preparing dataset mac_morpho/default to /home/andre/.cache/huggingface/datasets/mac_morpho/default/3.0.0/c87ce539abd72e76dd6b65403173c3e206ef0610d61bd7f487c438f2541a246b...\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Generating train split:   0%|          | 0/37948 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Generating test split:   0%|          | 0/9987 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Generating validation split:   0%|          | 0/1997 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset mac_morpho downloaded and prepared to /home/andre/.cache/huggingface/datasets/mac_morpho/default/3.0.0/c87ce539abd72e76dd6b65403173c3e206ef0610d61bd7f487c438f2541a246b. Subsequent calls will reuse this data.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Sequence(feature=ClassLabel(names=['PREP+PROADJ', 'IN', 'PREP+PRO-KS', 'NPROP', 'PREP+PROSUB', 'KC', 'PROPESS', 'NUM', 'PROADJ', 'PREP+ART', 'KS', 'PRO-KS', 'ADJ', 'ADV-KS', 'N', 'PREP', 'PROSUB', 'PREP+PROPESS', 'PDEN', 'V', 'PREP+ADV', 'PCP', 'CUR', 'ADV', 'PU', 'ART'], id=None), length=-1, id=None)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "macmorpho_dataset = load_dataset('mac_morpho')\n",
    "\n",
    "label_list = macmorpho_dataset['train'].features['pos_tags'].feature.names\n",
    "id2label = {idx: label for idx, label in enumerate(label_list)}\n",
    "label2id = {label: idx for idx, label in enumerate(label_list)}\n",
    "\n",
    "macmorpho_dataset['train'][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modelo utilizado e tokenização\n",
    "O modelo a ser treinado se trata de um fine-tuning do BERTimbau. O BERTimbau nada mais é do que uma arquitetura BERT pré-treinada com dados em português brasileiro. Como o nosso dataset se constitui de palavras em português brasileiro, o BERTimbau se mostrou como a alternativa mais interessante de modelo pré-treinado. Tendo o BERTimbau como base, foi treinado um modelo de classificação de tokens, o qual é adaptado para a classificação de palavras.\n",
    "\n",
    "O BERTimbau possui um tokenizer próprio, o qual também usamos neste trabalho. No entanto, surge o problema de que o tokenizer muitas vezes converte uma palavra em múltiplos tokens, tornando as POS-tags inconsistentes com o tamanho do vetor de tokens.\n",
    "\n",
    "Desse modo, como estamos utilizando um modelo de classificação de tokens e a tokenização não pode ser modificada devido ao pré-treinamento do modelo, utilizamos o tokenizador original e, para cada token gerado a partir de uma palavra do dataset, atribuímos a POS-tag da palavra aos tokens. Ao fim do treinamento, podemos transformar de volta as previsões dos tokens em previsões de palavras selecionando, para cada palavra, os logits de predição dos seus tokens correspondentes, tirando a média e utilizando o resultado para prever a POS-tag da palavra.\n",
    "\n",
    "O código a seguir tokeniza a base de dados e adapta o vetor de labels original à sua versão tokenizada. Os paddings são feitos sob demanda a cada batch utilizando um DataCollator, já que fazer o padding batch-a-batch diminuímos o tamanho médio dos vetores, melhorando a eficiência de memória."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "3LJq4FcHvKgo"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached processed dataset at /home/andre/.cache/huggingface/datasets/mac_morpho/default/3.0.0/c87ce539abd72e76dd6b65403173c3e206ef0610d61bd7f487c438f2541a246b/cache-a14d891a915977be.arrow\n",
      "Loading cached processed dataset at /home/andre/.cache/huggingface/datasets/mac_morpho/default/3.0.0/c87ce539abd72e76dd6b65403173c3e206ef0610d61bd7f487c438f2541a246b/cache-d1d917f37c9bfb0c.arrow\n",
      "Loading cached processed dataset at /home/andre/.cache/huggingface/datasets/mac_morpho/default/3.0.0/c87ce539abd72e76dd6b65403173c3e206ef0610d61bd7f487c438f2541a246b/cache-ebe39e72967b879a.arrow\n"
     ]
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained('neuralmind/bert-base-portuguese-cased', do_lower_case=False)\n",
    "\n",
    "def align_labels(words, labels, tokenizer):\n",
    "  token_labels = [[label for token in tokenizer.tokenize(word)] for word, label in zip(words, labels)]\n",
    "  return [-100] + sum(token_labels, []) + [-100]\n",
    "\n",
    "def tokenize_and_align_labels(examples):\n",
    "  tokenized_inputs = tokenizer(examples['tokens'], is_split_into_words=True)\n",
    "  tokenized_inputs['labels'] = [align_labels(words, labels, tokenizer) for words, labels in zip(examples['tokens'], examples['pos_tags'])]\n",
    "\n",
    "  return tokenized_inputs\n",
    "\n",
    "tokenized_dataset = macmorpho_dataset.map(tokenize_and_align_labels, batched=True)\n",
    "data_collator = DataCollatorForTokenClassification(tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Métricas de Avaliação Utilizadas na Validação\n",
    "\n",
    "Como forma de validar o modelo gerado, a seguinte função foi implementada. Ela basicamente utiliza o módulo seqeval para comparar as sequências de POS-tags geradas com as sequências verdadeiras, gerando acurácia, precisão, revocação e F1. Como o seqeval é projetado para a tarefa de NER, adicionamos o prefixo \"B-\" para que o método as reconheça. A adaptação é efetiva para POS-tags e documentada como alternativa de uso para tags não-NER no próprio github do pacote (https://github.com/chakki-works/seqeval/issues/75). A função é passada ao treinador do modelo e chamada a cada época.\n",
    "\n",
    "Note que as métricas calculadas são em relação à tarefa de POS-tagging de tokens, e não de palavras. Embora o mapeamento do problema de palavras para tokens tenha sido feito de maneira que ambos sejam muito semelhantes, ainda se trata de problemas diferentes. No entanto, no contexto de validação do modelo durante o treinamento, basta calcular as métricas para a classificação de tokens, já que o modelo treinado em si é um TokenClassifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "K9cS1m7WbM76"
   },
   "outputs": [],
   "source": [
    "seqeval = evaluate.load('seqeval')\n",
    "\n",
    "def compute_metrics(p):\n",
    "  prediction_logits, true_labels_idxs = p\n",
    "  predicted_labels_idxs = argmax(prediction_logits, axis=2)\n",
    "\n",
    "  predicted_labels = [[f'B-{label_list[pred_idx]}' for pred_idx, true_idx in zip(predicted_idxs, true_idxs) if true_idx != -100] for predicted_idxs, true_idxs in zip(predicted_labels_idxs, true_labels_idxs)]\n",
    "  true_labels = [[f'B-{label_list[true_idx]}' for true_idx in true_idxs if true_idx != -100] for true_idxs in true_labels_idxs]\n",
    "\n",
    "  results = seqeval.compute(predictions=predicted_labels, references=true_labels)\n",
    "\n",
    "  return {\n",
    "    \"precision\": results[\"overall_precision\"],\n",
    "    \"recall\": results[\"overall_recall\"],\n",
    "    \"f1\": results[\"overall_f1\"],\n",
    "    \"accuracy\": results[\"overall_accuracy\"],\n",
    "  }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hiperparâmetros do Modelos\n",
    "\n",
    "Conforme mencionado anteriormente, o modelo base utilizado é o BERTimbau. Como forma de otimizar o espaço alocado na GPU, a versão com pesos do tipo float32 foi utilizada. Como parâmetros do modelo, estou usando taxa de aprendizado 0.00002, 3 épocas, weight decay de 0.1 e tamanho de batch igual a 4. O tamanho de batch foi escolhido devido às limitações de memória, e a taxa de aprendizado, número de épocas e weight decay foram escolhidos usando como referência os minicursos de huggingface de Token Classification utilizando modelos baseados no BERT.\n",
    "\n",
    "No código a seguir, temos a instanciação do modelo, definição dos hiperparâmetros e inicialização do treinador com os dados de treino e validação. Na célula seguinte, o modelo é treinado."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 84,
     "referenced_widgets": [
      "b1aaf634969448ba8f5129b93657a22f",
      "ce203615f31945d4bd1837d3ed7f310c",
      "a5af0068b4d54d09aa6c9fad9f96be09",
      "17e78796af5246d7939ab788db3afd87",
      "1640d24cab4b4448bf309928fd701b56",
      "d600566bfe604370812b787dba35fcf1",
      "6ad7bf077afe488593d1543fe06012c4",
      "254b46bf01874c61b1d5674a2fa2cd0c",
      "89ac0231d288472b8044e8b62a609e82",
      "379c550f7adf438283d956df577fd150",
      "715fb7da1588484b96838049d37b179e"
     ]
    },
    "id": "QqF5Lr8TLg4Y",
    "outputId": "979492cb-9c30-4589-e168-b853e7707094"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at neuralmind/bert-base-portuguese-cased were not used when initializing BertForTokenClassification: ['cls.seq_relationship.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.weight', 'cls.predictions.decoder.weight']\n",
      "- This IS expected if you are initializing BertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForTokenClassification were not initialized from the model checkpoint at neuralmind/bert-base-portuguese-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "torch.cuda.empty_cache()\n",
    "\n",
    "model = AutoModelForTokenClassification.from_pretrained('neuralmind/bert-base-portuguese-cased', torch_dtype=torch.float32, num_labels = len(label_list), id2label = id2label, label2id = label2id)\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"model_results\",\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=4,\n",
    "    per_device_eval_batch_size=4,\n",
    "    num_train_epochs=3,\n",
    "    weight_decay=0.01,\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    load_best_model_at_end=True,\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_dataset[\"train\"],\n",
    "    eval_dataset=tokenized_dataset[\"validation\"],\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=data_collator,\n",
    "    compute_metrics=compute_metrics,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 436
    },
    "id": "jYZY0SN5NGvQ",
    "outputId": "cc6b0756-b341-4cf2-b94f-887c7044fd49"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/andre/miniconda3/envs/notebook_env/lib/python3.10/site-packages/transformers/optimization.py:407: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "  0%|          | 0/28461 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.5574, 'learning_rate': 1.9648642001335163e-05, 'epoch': 0.05}\n",
      "{'loss': 0.1406, 'learning_rate': 1.929728400267032e-05, 'epoch': 0.11}\n",
      "{'loss': 0.1133, 'learning_rate': 1.8945926004005485e-05, 'epoch': 0.16}\n",
      "{'loss': 0.1158, 'learning_rate': 1.8594568005340643e-05, 'epoch': 0.21}\n",
      "{'loss': 0.099, 'learning_rate': 1.8243210006675805e-05, 'epoch': 0.26}\n",
      "{'loss': 0.0943, 'learning_rate': 1.7891852008010962e-05, 'epoch': 0.32}\n",
      "{'loss': 0.0863, 'learning_rate': 1.7540494009346124e-05, 'epoch': 0.37}\n",
      "{'loss': 0.0929, 'learning_rate': 1.7189136010681285e-05, 'epoch': 0.42}\n",
      "{'loss': 0.0826, 'learning_rate': 1.6837778012016446e-05, 'epoch': 0.47}\n",
      "{'loss': 0.0791, 'learning_rate': 1.6486420013351604e-05, 'epoch': 0.53}\n",
      "{'loss': 0.0817, 'learning_rate': 1.6135062014686765e-05, 'epoch': 0.58}\n",
      "{'loss': 0.0815, 'learning_rate': 1.5783704016021927e-05, 'epoch': 0.63}\n",
      "{'loss': 0.0774, 'learning_rate': 1.5432346017357088e-05, 'epoch': 0.69}\n",
      "{'loss': 0.0829, 'learning_rate': 1.5080988018692246e-05, 'epoch': 0.74}\n",
      "{'loss': 0.0813, 'learning_rate': 1.4729630020027407e-05, 'epoch': 0.79}\n",
      "{'loss': 0.0756, 'learning_rate': 1.4378272021362567e-05, 'epoch': 0.84}\n",
      "{'loss': 0.0715, 'learning_rate': 1.4026914022697728e-05, 'epoch': 0.9}\n",
      "{'loss': 0.0812, 'learning_rate': 1.3675556024032887e-05, 'epoch': 0.95}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "  0%|          | 0/500 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/andre/miniconda3/envs/notebook_env/lib/python3.10/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: ADV seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
      "/home/andre/miniconda3/envs/notebook_env/lib/python3.10/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: PREP seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
      "/home/andre/miniconda3/envs/notebook_env/lib/python3.10/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: N seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
      "/home/andre/miniconda3/envs/notebook_env/lib/python3.10/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: PU seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
      "/home/andre/miniconda3/envs/notebook_env/lib/python3.10/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: V seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
      "/home/andre/miniconda3/envs/notebook_env/lib/python3.10/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: PCP seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
      "/home/andre/miniconda3/envs/notebook_env/lib/python3.10/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: ART seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
      "/home/andre/miniconda3/envs/notebook_env/lib/python3.10/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: ADJ seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
      "/home/andre/miniconda3/envs/notebook_env/lib/python3.10/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: PRO-KS seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
      "/home/andre/miniconda3/envs/notebook_env/lib/python3.10/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: PREP+ART seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
      "/home/andre/miniconda3/envs/notebook_env/lib/python3.10/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: NPROP seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
      "/home/andre/miniconda3/envs/notebook_env/lib/python3.10/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: KC seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
      "/home/andre/miniconda3/envs/notebook_env/lib/python3.10/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: PROADJ seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
      "/home/andre/miniconda3/envs/notebook_env/lib/python3.10/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: KS seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
      "/home/andre/miniconda3/envs/notebook_env/lib/python3.10/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: PROPESS seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
      "/home/andre/miniconda3/envs/notebook_env/lib/python3.10/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: PDEN seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
      "/home/andre/miniconda3/envs/notebook_env/lib/python3.10/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: NUM seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
      "/home/andre/miniconda3/envs/notebook_env/lib/python3.10/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: PREP+PROADJ seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
      "/home/andre/miniconda3/envs/notebook_env/lib/python3.10/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: CUR seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
      "/home/andre/miniconda3/envs/notebook_env/lib/python3.10/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: PROSUB seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
      "/home/andre/miniconda3/envs/notebook_env/lib/python3.10/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: PREP+PROSUB seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
      "/home/andre/miniconda3/envs/notebook_env/lib/python3.10/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: ADV-KS seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
      "/home/andre/miniconda3/envs/notebook_env/lib/python3.10/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: PREP+PROPESS seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
      "/home/andre/miniconda3/envs/notebook_env/lib/python3.10/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: IN seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
      "/home/andre/miniconda3/envs/notebook_env/lib/python3.10/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: PREP+PRO-KS seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
      "/home/andre/miniconda3/envs/notebook_env/lib/python3.10/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: PREP+ADV seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.06989305466413498, 'eval_precision': 0.9771771597598817, 'eval_recall': 0.9780280381969639, 'eval_f1': 0.9776024138331205, 'eval_accuracy': 0.9813507421426445, 'eval_runtime': 12.0197, 'eval_samples_per_second': 166.144, 'eval_steps_per_second': 41.598, 'epoch': 1.0}\n",
      "{'loss': 0.0714, 'learning_rate': 1.3324198025368049e-05, 'epoch': 1.0}\n",
      "{'loss': 0.0527, 'learning_rate': 1.2972840026703208e-05, 'epoch': 1.05}\n",
      "{'loss': 0.0543, 'learning_rate': 1.262148202803837e-05, 'epoch': 1.11}\n",
      "{'loss': 0.0529, 'learning_rate': 1.2270124029373529e-05, 'epoch': 1.16}\n",
      "{'loss': 0.0525, 'learning_rate': 1.191876603070869e-05, 'epoch': 1.21}\n",
      "{'loss': 0.0526, 'learning_rate': 1.156740803204385e-05, 'epoch': 1.26}\n",
      "{'loss': 0.0548, 'learning_rate': 1.1216050033379011e-05, 'epoch': 1.32}\n",
      "{'loss': 0.0513, 'learning_rate': 1.086469203471417e-05, 'epoch': 1.37}\n",
      "{'loss': 0.0507, 'learning_rate': 1.0513334036049332e-05, 'epoch': 1.42}\n",
      "{'loss': 0.0588, 'learning_rate': 1.0161976037384492e-05, 'epoch': 1.48}\n",
      "{'loss': 0.053, 'learning_rate': 9.810618038719653e-06, 'epoch': 1.53}\n",
      "{'loss': 0.0508, 'learning_rate': 9.459260040054812e-06, 'epoch': 1.58}\n",
      "{'loss': 0.0515, 'learning_rate': 9.107902041389974e-06, 'epoch': 1.63}\n",
      "{'loss': 0.0434, 'learning_rate': 8.756544042725133e-06, 'epoch': 1.69}\n",
      "{'loss': 0.0474, 'learning_rate': 8.405186044060295e-06, 'epoch': 1.74}\n",
      "{'loss': 0.0485, 'learning_rate': 8.053828045395454e-06, 'epoch': 1.79}\n",
      "{'loss': 0.0436, 'learning_rate': 7.702470046730615e-06, 'epoch': 1.84}\n",
      "{'loss': 0.052, 'learning_rate': 7.351112048065775e-06, 'epoch': 1.9}\n",
      "{'loss': 0.047, 'learning_rate': 6.999754049400935e-06, 'epoch': 1.95}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "  0%|          | 0/500 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/andre/miniconda3/envs/notebook_env/lib/python3.10/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: ADV seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
      "/home/andre/miniconda3/envs/notebook_env/lib/python3.10/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: PREP seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
      "/home/andre/miniconda3/envs/notebook_env/lib/python3.10/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: N seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
      "/home/andre/miniconda3/envs/notebook_env/lib/python3.10/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: PU seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
      "/home/andre/miniconda3/envs/notebook_env/lib/python3.10/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: V seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
      "/home/andre/miniconda3/envs/notebook_env/lib/python3.10/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: PCP seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
      "/home/andre/miniconda3/envs/notebook_env/lib/python3.10/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: ART seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
      "/home/andre/miniconda3/envs/notebook_env/lib/python3.10/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: ADJ seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
      "/home/andre/miniconda3/envs/notebook_env/lib/python3.10/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: PRO-KS seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
      "/home/andre/miniconda3/envs/notebook_env/lib/python3.10/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: PREP+ART seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
      "/home/andre/miniconda3/envs/notebook_env/lib/python3.10/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: NPROP seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
      "/home/andre/miniconda3/envs/notebook_env/lib/python3.10/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: KC seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
      "/home/andre/miniconda3/envs/notebook_env/lib/python3.10/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: PROADJ seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
      "/home/andre/miniconda3/envs/notebook_env/lib/python3.10/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: KS seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
      "/home/andre/miniconda3/envs/notebook_env/lib/python3.10/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: PROPESS seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
      "/home/andre/miniconda3/envs/notebook_env/lib/python3.10/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: PDEN seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
      "/home/andre/miniconda3/envs/notebook_env/lib/python3.10/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: NUM seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
      "/home/andre/miniconda3/envs/notebook_env/lib/python3.10/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: PREP+PROADJ seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
      "/home/andre/miniconda3/envs/notebook_env/lib/python3.10/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: CUR seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
      "/home/andre/miniconda3/envs/notebook_env/lib/python3.10/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: PROSUB seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
      "/home/andre/miniconda3/envs/notebook_env/lib/python3.10/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: PREP+PROSUB seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
      "/home/andre/miniconda3/envs/notebook_env/lib/python3.10/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: ADV-KS seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
      "/home/andre/miniconda3/envs/notebook_env/lib/python3.10/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: PREP+PROPESS seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
      "/home/andre/miniconda3/envs/notebook_env/lib/python3.10/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: IN seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
      "/home/andre/miniconda3/envs/notebook_env/lib/python3.10/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: PREP+PRO-KS seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
      "/home/andre/miniconda3/envs/notebook_env/lib/python3.10/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: PREP+ADV seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.0712089091539383, 'eval_precision': 0.9800917058447965, 'eval_recall': 0.9802339418918526, 'eval_f1': 0.9801628187081887, 'eval_accuracy': 0.982903074894739, 'eval_runtime': 12.0035, 'eval_samples_per_second': 166.369, 'eval_steps_per_second': 41.655, 'epoch': 2.0}\n",
      "{'loss': 0.0472, 'learning_rate': 6.648396050736096e-06, 'epoch': 2.0}\n",
      "{'loss': 0.0328, 'learning_rate': 6.297038052071256e-06, 'epoch': 2.06}\n",
      "{'loss': 0.0276, 'learning_rate': 5.945680053406417e-06, 'epoch': 2.11}\n",
      "{'loss': 0.0326, 'learning_rate': 5.594322054741577e-06, 'epoch': 2.16}\n",
      "{'loss': 0.0347, 'learning_rate': 5.242964056076737e-06, 'epoch': 2.21}\n",
      "{'loss': 0.0304, 'learning_rate': 4.891606057411897e-06, 'epoch': 2.27}\n",
      "{'loss': 0.0317, 'learning_rate': 4.5402480587470574e-06, 'epoch': 2.32}\n",
      "{'loss': 0.0301, 'learning_rate': 4.188890060082218e-06, 'epoch': 2.37}\n",
      "{'loss': 0.0328, 'learning_rate': 3.837532061417378e-06, 'epoch': 2.42}\n",
      "{'loss': 0.0303, 'learning_rate': 3.4861740627525387e-06, 'epoch': 2.48}\n",
      "{'loss': 0.0331, 'learning_rate': 3.134816064087699e-06, 'epoch': 2.53}\n",
      "{'loss': 0.0301, 'learning_rate': 2.7834580654228595e-06, 'epoch': 2.58}\n",
      "{'loss': 0.0308, 'learning_rate': 2.43210006675802e-06, 'epoch': 2.64}\n",
      "{'loss': 0.0329, 'learning_rate': 2.0807420680931804e-06, 'epoch': 2.69}\n",
      "{'loss': 0.0349, 'learning_rate': 1.7293840694283406e-06, 'epoch': 2.74}\n",
      "{'loss': 0.0282, 'learning_rate': 1.378026070763501e-06, 'epoch': 2.79}\n",
      "{'loss': 0.0246, 'learning_rate': 1.0266680720986614e-06, 'epoch': 2.85}\n",
      "{'loss': 0.0297, 'learning_rate': 6.753100734338218e-07, 'epoch': 2.9}\n",
      "{'loss': 0.0322, 'learning_rate': 3.2395207476898214e-07, 'epoch': 2.95}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "  0%|          | 0/500 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/andre/miniconda3/envs/notebook_env/lib/python3.10/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: ADV seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
      "/home/andre/miniconda3/envs/notebook_env/lib/python3.10/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: PREP seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
      "/home/andre/miniconda3/envs/notebook_env/lib/python3.10/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: N seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
      "/home/andre/miniconda3/envs/notebook_env/lib/python3.10/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: PU seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
      "/home/andre/miniconda3/envs/notebook_env/lib/python3.10/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: V seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
      "/home/andre/miniconda3/envs/notebook_env/lib/python3.10/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: PCP seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
      "/home/andre/miniconda3/envs/notebook_env/lib/python3.10/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: ART seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
      "/home/andre/miniconda3/envs/notebook_env/lib/python3.10/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: ADJ seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
      "/home/andre/miniconda3/envs/notebook_env/lib/python3.10/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: PRO-KS seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
      "/home/andre/miniconda3/envs/notebook_env/lib/python3.10/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: PREP+ART seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
      "/home/andre/miniconda3/envs/notebook_env/lib/python3.10/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: NPROP seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
      "/home/andre/miniconda3/envs/notebook_env/lib/python3.10/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: KC seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
      "/home/andre/miniconda3/envs/notebook_env/lib/python3.10/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: PROADJ seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
      "/home/andre/miniconda3/envs/notebook_env/lib/python3.10/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: KS seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
      "/home/andre/miniconda3/envs/notebook_env/lib/python3.10/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: PROPESS seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
      "/home/andre/miniconda3/envs/notebook_env/lib/python3.10/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: PDEN seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
      "/home/andre/miniconda3/envs/notebook_env/lib/python3.10/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: NUM seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
      "/home/andre/miniconda3/envs/notebook_env/lib/python3.10/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: PREP+PROADJ seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
      "/home/andre/miniconda3/envs/notebook_env/lib/python3.10/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: CUR seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
      "/home/andre/miniconda3/envs/notebook_env/lib/python3.10/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: PROSUB seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
      "/home/andre/miniconda3/envs/notebook_env/lib/python3.10/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: PREP+PROSUB seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
      "/home/andre/miniconda3/envs/notebook_env/lib/python3.10/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: ADV-KS seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
      "/home/andre/miniconda3/envs/notebook_env/lib/python3.10/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: PREP+PROPESS seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
      "/home/andre/miniconda3/envs/notebook_env/lib/python3.10/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: IN seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
      "/home/andre/miniconda3/envs/notebook_env/lib/python3.10/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: PREP+PRO-KS seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
      "/home/andre/miniconda3/envs/notebook_env/lib/python3.10/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: PREP+ADV seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.07421258091926575, 'eval_precision': 0.9804769087955442, 'eval_recall': 0.9810176182045105, 'eval_f1': 0.9807471889735219, 'eval_accuracy': 0.9834984901969124, 'eval_runtime': 12.009, 'eval_samples_per_second': 166.292, 'eval_steps_per_second': 41.636, 'epoch': 3.0}\n",
      "{'train_runtime': 3552.2484, 'train_samples_per_second': 32.048, 'train_steps_per_second': 8.012, 'train_loss': 0.06540923733297585, 'epoch': 3.0}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "  0%|          | 0/500 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/andre/miniconda3/envs/notebook_env/lib/python3.10/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: ADV seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
      "/home/andre/miniconda3/envs/notebook_env/lib/python3.10/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: PREP seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
      "/home/andre/miniconda3/envs/notebook_env/lib/python3.10/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: N seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
      "/home/andre/miniconda3/envs/notebook_env/lib/python3.10/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: PU seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
      "/home/andre/miniconda3/envs/notebook_env/lib/python3.10/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: V seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
      "/home/andre/miniconda3/envs/notebook_env/lib/python3.10/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: PCP seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
      "/home/andre/miniconda3/envs/notebook_env/lib/python3.10/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: ART seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
      "/home/andre/miniconda3/envs/notebook_env/lib/python3.10/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: ADJ seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
      "/home/andre/miniconda3/envs/notebook_env/lib/python3.10/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: PRO-KS seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
      "/home/andre/miniconda3/envs/notebook_env/lib/python3.10/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: PREP+ART seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
      "/home/andre/miniconda3/envs/notebook_env/lib/python3.10/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: NPROP seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
      "/home/andre/miniconda3/envs/notebook_env/lib/python3.10/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: KC seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
      "/home/andre/miniconda3/envs/notebook_env/lib/python3.10/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: PROADJ seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
      "/home/andre/miniconda3/envs/notebook_env/lib/python3.10/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: KS seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
      "/home/andre/miniconda3/envs/notebook_env/lib/python3.10/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: PROPESS seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
      "/home/andre/miniconda3/envs/notebook_env/lib/python3.10/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: PDEN seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
      "/home/andre/miniconda3/envs/notebook_env/lib/python3.10/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: NUM seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
      "/home/andre/miniconda3/envs/notebook_env/lib/python3.10/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: PREP+PROADJ seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
      "/home/andre/miniconda3/envs/notebook_env/lib/python3.10/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: CUR seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
      "/home/andre/miniconda3/envs/notebook_env/lib/python3.10/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: PROSUB seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
      "/home/andre/miniconda3/envs/notebook_env/lib/python3.10/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: PREP+PROSUB seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
      "/home/andre/miniconda3/envs/notebook_env/lib/python3.10/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: ADV-KS seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
      "/home/andre/miniconda3/envs/notebook_env/lib/python3.10/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: PREP+PROPESS seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
      "/home/andre/miniconda3/envs/notebook_env/lib/python3.10/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: IN seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
      "/home/andre/miniconda3/envs/notebook_env/lib/python3.10/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: PREP+PRO-KS seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
      "/home/andre/miniconda3/envs/notebook_env/lib/python3.10/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: PREP+ADV seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'eval_loss': 0.06989305466413498,\n",
       " 'eval_precision': 0.9771771597598817,\n",
       " 'eval_recall': 0.9780280381969639,\n",
       " 'eval_f1': 0.9776024138331205,\n",
       " 'eval_accuracy': 0.9813507421426445,\n",
       " 'eval_runtime': 11.9688,\n",
       " 'eval_samples_per_second': 166.851,\n",
       " 'eval_steps_per_second': 41.775,\n",
       " 'epoch': 3.0}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()\n",
    "results = trainer.evaluate()\n",
    "results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modelo Resultante\n",
    "\n",
    "Conforme observado no treinamento acima, os resultados sugerem muito bons resultados, com o modelo apresentando acurácia de mais de 98% de validação. No entanto, precisamos verificar o desempenho de fato aplicando o teste. Para isso, temos um modelo treinado com os hiperparâmetros mencionados anteriormente salvos no diretório definido na variável best_model.\n",
    "\n",
    "O código a seguir faz o carregamento deste modelo, junto ao tokenizer e o dataset. Carregamos o dataset e o tokenizer novamente para que ambos estejam no mesmo ambiente do modelo e para que o dataset original seja utilizado para o teste, e não o tokenizado utilizado para o treinamento."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset mac_morpho (/home/andre/.cache/huggingface/datasets/mac_morpho/default/3.0.0/c87ce539abd72e76dd6b65403173c3e206ef0610d61bd7f487c438f2541a246b)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{'id': '0', 'tokens': ['Salto', 'sete'], 'pos_tags': [14, 12]}"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_model = 'model_results/checkpoint-28461'\n",
    "loaded_model = AutoModelForTokenClassification.from_pretrained(best_model)\n",
    "tokenizer = AutoTokenizer.from_pretrained('neuralmind/bert-base-portuguese-cased', do_lower_case=False,  return_tensors=\"pt\")\n",
    "\n",
    "macmorpho_dataset = load_dataset('mac_morpho')\n",
    "label_list = macmorpho_dataset['train'].features['pos_tags'].feature.names\n",
    "\n",
    "macmorpho_dataset['test'][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inferência e Teste\n",
    "\n",
    "Conforme explicado nas células anteriores, o modelo é o classificador de tokens, e não de palavras. Para isso, na inferência aplicamos a tokenização e obtemos os logits de predição de cada POS-tag para cada token. Em seguida, para cada palavra da entrada, verificamos quantos tokens são gerados para aquela palavra após a tokenização. Então, selecionamos os logits correspondentes aos k tokens oriundos daquela palavra e tiramos a média, gerando assim o logit de predição da palavra. Fazemos isso para cada palavra da entrada. Com os logits de cada palavra, selecionamos a POS-tag de maior probabilidade e atribuímos à palavra.\n",
    "\n",
    "O teste do modelo foi feito utilizando a base original, e portanto as labels não foram adaptadas para o vetor de tokens. Como forma de avaliar o modelo de maneira fiel à sua função de inferência, o mesmo foi testado para a classificação de palavras utilizando a técnica definida anteriormente, ao contrário da avaliação da classificação token-a-token. Desse modo, para cada sentença da entrada, realizamos a inferência usando a função acima e, ao fim, calculamos as métricas das labels previstas em relação às labels originais, sem adaptação, da base.\n",
    "\n",
    "O código a seguir implementa as funções de inferência e teste, e testa o modelo implementado contra a base de teste do Mac-Morph:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'precision': 0.9813144366019521,\n",
       " 'recall': 0.9813144366019521,\n",
       " 'f1': 0.9813144366019521,\n",
       " 'accuracy': 0.9813144366019521}"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "seqeval = evaluate.load('seqeval')\n",
    "\n",
    "#Função de inferência de POS-tags dada uma sentença, usando o modelo treinado\n",
    "def predict_POS_tags(sentence, model, tokenizer, is_split_into_words = False):\n",
    "  words = sentence if is_split_into_words else sentence.split()\n",
    "  input = tokenizer(sentence, return_tensors='pt', is_split_into_words=is_split_into_words)\n",
    "  \n",
    "  #Realizamos a inferência dos logits (probabilidade de cada label)\n",
    "  with torch.no_grad():\n",
    "    logits = model(**input).logits[0]\n",
    "\n",
    "  logits = logits[1:-1]\n",
    "\n",
    "  #Convertemos tokens para palavras. Para cada palavra, selecionamos os k tokens aos quais ela corresponde após a tokenização e tiramos a média dos logits\n",
    "  word_logits = []\n",
    "  token_nums = [len(tokenizer.tokenize(word)) for word in words if len(tokenizer.tokenize(word)) != 0]\n",
    "  tok_idx = 0\n",
    "\n",
    "  for token_num in token_nums:\n",
    "    word_logit = torch.mean(logits[tok_idx: tok_idx + token_num], dim=0)\n",
    "    word_logits.append(word_logit)\n",
    "    tok_idx += token_num\n",
    "\n",
    "  #Para cada palavra, selecionamos o índice da label de maior probabilidade, convertemos o índice para o nome da label e geramos a saída\n",
    "  predicted_labels_idxs = argmax(word_logits, axis=1)\n",
    "\n",
    "  predicted_labels = [label_list[idx] for idx in predicted_labels_idxs]\n",
    "\n",
    "  return predicted_labels\n",
    "\n",
    "#Função de teste do modelo. Este teste leva em conta a inferência de POS tags por palavra, não por token como durante o treino\n",
    "def test_POS_tags_model(test_dataset, model, tokenizer, is_split_into_words = True):\n",
    "  true_POS_tags = [[f'B-{label_list[label_idx]}' for label_idx in label] for label in test_dataset['pos_tags']]\n",
    "  predicted_POS_tags = [predict_POS_tags(words, model, tokenizer, is_split_into_words) for words in test_dataset['tokens']]\n",
    "  predicted_POS_tags = [[f'B-{lab}' for lab in label] for label in predicted_POS_tags]\n",
    "\n",
    "  results = seqeval.compute(predictions=predicted_POS_tags, references= true_POS_tags)\n",
    "\n",
    "  overall_results = {\n",
    "    \"precision\": results[\"overall_precision\"],\n",
    "    \"recall\": results[\"overall_recall\"],\n",
    "    \"f1\": results[\"overall_f1\"],\n",
    "    \"accuracy\": results[\"overall_accuracy\"],\n",
    "  }\n",
    "\n",
    "  per_tag_precisions = {tag: results[tag]['precision'] for tag in label_list}\n",
    "\n",
    "  return overall_results, per_tag_precisions\n",
    "\n",
    "overall_results, per_tag_precisions = test_POS_tags_model(macmorpho_dataset['test'], loaded_model, tokenizer)\n",
    "overall_results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Resultados\n",
    "\n",
    "Como podemos ver, o modelo se mostrou extremamente efetivo na tarefa de POS-tagging, apresentando acurácia acima de 98% no teste. A precisão, recall e f1 acima de 97% sugerem, também, que este resultado não é desbalanceado entre as POS-tags classificadas, por serem valores também extremamente altos. Vamos observar a precisão de cada label no código a seguir:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precisões totais:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>precision</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>PREP+PROPESS</th>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>PU</th>\n",
       "      <td>0.999777</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>PREP+PROADJ</th>\n",
       "      <td>0.996753</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>CUR</th>\n",
       "      <td>0.996622</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>V</th>\n",
       "      <td>0.995837</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>PROPESS</th>\n",
       "      <td>0.994753</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>PREP+ART</th>\n",
       "      <td>0.993051</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ART</th>\n",
       "      <td>0.989786</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>KC</th>\n",
       "      <td>0.988649</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>N</th>\n",
       "      <td>0.981395</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>PREP</th>\n",
       "      <td>0.981176</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>NPROP</th>\n",
       "      <td>0.977380</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>PCP</th>\n",
       "      <td>0.974572</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>NUM</th>\n",
       "      <td>0.974400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>PROADJ</th>\n",
       "      <td>0.973068</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>PREP+ADV</th>\n",
       "      <td>0.964286</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ADJ</th>\n",
       "      <td>0.960704</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>KS</th>\n",
       "      <td>0.940726</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ADV</th>\n",
       "      <td>0.938101</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>PRO-KS</th>\n",
       "      <td>0.926851</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>PROSUB</th>\n",
       "      <td>0.921824</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>PREP+PROSUB</th>\n",
       "      <td>0.906667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>PDEN</th>\n",
       "      <td>0.885740</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>PREP+PRO-KS</th>\n",
       "      <td>0.870968</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ADV-KS</th>\n",
       "      <td>0.847826</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>IN</th>\n",
       "      <td>0.480263</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              precision\n",
       "PREP+PROPESS   1.000000\n",
       "PU             0.999777\n",
       "PREP+PROADJ    0.996753\n",
       "CUR            0.996622\n",
       "V              0.995837\n",
       "PROPESS        0.994753\n",
       "PREP+ART       0.993051\n",
       "ART            0.989786\n",
       "KC             0.988649\n",
       "N              0.981395\n",
       "PREP           0.981176\n",
       "NPROP          0.977380\n",
       "PCP            0.974572\n",
       "NUM            0.974400\n",
       "PROADJ         0.973068\n",
       "PREP+ADV       0.964286\n",
       "ADJ            0.960704\n",
       "KS             0.940726\n",
       "ADV            0.938101\n",
       "PRO-KS         0.926851\n",
       "PROSUB         0.921824\n",
       "PREP+PROSUB    0.906667\n",
       "PDEN           0.885740\n",
       "PREP+PRO-KS    0.870968\n",
       "ADV-KS         0.847826\n",
       "IN             0.480263"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Maiores precisões:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>precision</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>PREP+PROPESS</th>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>PU</th>\n",
       "      <td>0.999777</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>PREP+PROADJ</th>\n",
       "      <td>0.996753</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>CUR</th>\n",
       "      <td>0.996622</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>V</th>\n",
       "      <td>0.995837</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              precision\n",
       "PREP+PROPESS   1.000000\n",
       "PU             0.999777\n",
       "PREP+PROADJ    0.996753\n",
       "CUR            0.996622\n",
       "V              0.995837"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Menores precisões:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>precision</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>PREP+PROSUB</th>\n",
       "      <td>0.906667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>PDEN</th>\n",
       "      <td>0.885740</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>PREP+PRO-KS</th>\n",
       "      <td>0.870968</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ADV-KS</th>\n",
       "      <td>0.847826</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>IN</th>\n",
       "      <td>0.480263</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             precision\n",
       "PREP+PROSUB   0.906667\n",
       "PDEN          0.885740\n",
       "PREP+PRO-KS   0.870968\n",
       "ADV-KS        0.847826\n",
       "IN            0.480263"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['PREP+PROADJ', 'IN', 'PREP+PRO-KS', 'NPROP', 'PREP+PROSUB', 'KC', 'PROPESS', 'NUM', 'PROADJ', 'PREP+ART', 'KS', 'PRO-KS', 'ADJ', 'ADV-KS', 'N', 'PREP', 'PROSUB', 'PREP+PROPESS', 'PDEN', 'V', 'PREP+ADV', 'PCP', 'CUR', 'ADV', 'PU', 'ART']\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "s = pd.Series(per_tag_precisions)\n",
    "df = pd.DataFrame(s, columns=['precision'])\n",
    "sorted_df = df.sort_values('precision', ascending = False)\n",
    "print('Precisões totais:')\n",
    "display(sorted_df)\n",
    "\n",
    "print('Maiores precisões:')\n",
    "display(sorted_df.iloc[:5])\n",
    "\n",
    "print('Menores precisões:')\n",
    "display(sorted_df.iloc[-5:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Precisões Por Classe\n",
    "\n",
    "Observando as precisões de cada classe, podemos observar que, das 26 POS-tags, 21 apresentaram precisão acima de 90%, indicando um ótimo resultado geral para a maioria das tags. Observando as 5 maiores precisões, todas foram maiores que 99%, para as tags correspondentes às respectivas classes gramaticais:\n",
    "\n",
    "* Preposição + Pronome Pessoal (PREP+PROPESS), ex. nela, dele\n",
    "* Pontuação (PU) ex. !, ?, .\n",
    "* Preposição + Pronome Adjetivo (PREP+PROADJ), ex. nesse, daquele\n",
    "* Símbolo de Moeda Corrente (CUR), ex. R$, £\n",
    "* Verbo (V), ex. Cantarei, Falhou\n",
    "\n",
    "É notável que as quatro tags de maior precisão correspondem a palavras/tokens que, num geral, independem do contexto para serem classificadas. Embora uma mesma palavra possa assumir diferentes classes gramaticais em diferentes contextos, algumas palavras são usadas exclusivamente, ou majoritariamente, para uma única classe específica. Um bom exemplo disso que pode ser observado nas classes acima são os símbolos de moeda corrente e as pontuações. Os contextos nos quais os tokens correspondentes a estas classes são utilizados são notavelmente quase que exclusivos a elas. O mesmo ocorre para as preposições. Os verbos, embora possam assumir outros papéis gramaticais (ex. em \"O cantar dos pássaros\" a palavra \"cantar\" assume posição de substantivo), também assumem posição majoritariamente de verbos.\n",
    "\n",
    "Vamos observar agora as preposições de menor precisão:\n",
    "\n",
    "* Preposição + Pronome Substantivo (PREP+PROSUB) ex. \"Disso_PREP+POSUB você entende\"\n",
    "* Palavra Denotativa (PDEN), ex. \"Então_PDEN quem foi?\"\n",
    "* Preposição + Pronome Conectivo Subordinativo (PREP+PRO-KS) ex. \"A cidade à=qual_PREP+PRO-KS fui\"\n",
    "* Advérbio Conectivo Subordinativo(ADV-KS) ex. \"Sei como_ADV-KS resolver o problema\"\n",
    "* Interjeição (IN), ex. \"Bom dia\", \"nossa\", \"Ô de casa\"\n",
    "\n",
    "Como podemos ver acima, as classes de menor precisão são, também, as que mais dependem do contexto. Por exemplo, \"Palavra Denotativa\" é a descrição dada a uma palavra que geralmente assumiria posição de advérbio, mas que no contexto da frase não modifica nenhum adjetivo ou verbo. Desse modo, todas os tokens correspondentes a palavras-denotativas podem também assumir posição de advérbio. O mesmo ocorre para advérbios conectivos subordinativos, que são advérbios que assumem também o papel gramatical de introduzir uma oração subordinativa. Algo semelhante ocorre com a Preposição + Pronome Substantivo e a Preposição + Pronome Conectivo Subordinativo, que correspondem aos mesmos tokens de Preposição + Pronome (ex. PREP+PROADJ, PREP+PRO) porém no contexto gramatical específico de assumir posição substantiva (como sujeito ou objeto) ou introduzir orações subordinativas. Com isso, é esperado que a precisão destas classes, comparada as demais, seja mais baixa, uma vez que elas dependem quase que exclusivamente dos mecanismos de atenção para a classificação, já que compartilham possíveis tokens comm muitas outras classes. Mesmo assim, para as classes citadas, o modelo ainda teve precisão acima de 84%, o que ainda é considerado bom, evidenciando a importância dos mecanismos de atenção neste caso.\n",
    "\n",
    "É notável, no entanto, que a classe de interjeições teve uma precisão consideravelmente baixa, não só comparada às demais classes, como de maneira geral tamém, ficando abaixo de 50%, que seria a predição aleatória. Por um lado, as interjeições sofrem dos mesmos problemas mencionados nas classes anteriores, uma vez que, com a excceção de gírias, todas as interjeições e locuções interjeitivas são formadas por palavras que podem assumir outras classes gramaticais. Isso ocorre pois interjeições correspondem a palavras ou frases usadas para manifestar emoções súbitas. Então qualquer palavra ou frase pode assumir o papel de uma interjeição, inclusive palavras quase que exclusivas a uma classe gramatical específica, como mencionado ao descrever as maiores precisões. No entanto, também é possível que o método de tokenização tenha prejudicado a representação do modelo para interjeições. Por um lado, por algumas interjeições corresponderem a gírias, elas não estão no vocabulário do BERTimbau, sendo decompostas em sequências menores e mesmo caractere-a-caractere. Por outro, é muito comum que interjeições ocorram no começo da frase, e o BERTimbau só possui palavras completas em letras minúsculas em seu vocabulário. Desse modo, sempre que uma palavra começa com uma letra maiúscula, ela é decomposta em sub-tokens, o que pode ter sido um fator limitante no caso das interjeições.\n",
    "\n",
    "Por fim, podemos concluir que o modelo proposto apresentou um ótimo desempenho não só de maneira geral, como também classe-a-classe, apresentando precisão acima de 90% para 80% das tags e precisão acima de 80% para 96% das tags. Podemos concluir, portanto, que o modelo resolve de maneira efetiva a tarefa de POS-tagging de sentenças da língua portuguesa na grande maioria dos casos de uso."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
