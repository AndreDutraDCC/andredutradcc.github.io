{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Trabalho Final\n",
    "## Detecção de cláusulas potencialmente injustas em contratos de Termos de Serviço utilizando fine-tuning no LEGAL-BERT\n",
    "\n",
    "##### **Nome:** André Luiz Moreira Dutra\n",
    "##### **Matrícula:** 2019006345\n",
    "\n",
    "#### Linl da Apresentação: https://youtu.be/igoGCZgvFng"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Motivação\n",
    "\n",
    "Com o atual crescente avanço tecnológico, cada vez mais recursos e funcionalidades vêm se tornando acessíveis ao usuário comum por meio de aplicativos, websites e servidores. Legalmente, o oferecimento destes recursos é reconhecido como uma prestação de serviços e, com isso, requer que um contrato formal de Termos de Serviço seja firmado, com o qual todas as partes envolvidas, inclusive o usuário final, devem concordar.\n",
    "\n",
    "No entanto, a demanda por serviços cada vez mais rápidos e de fácil acesso tem diminuido cada vez mais o interesse dos usuários em ler estes contratos e garantir que seus direitos são garantidos. Unindo isso ao crescente interesse de empresas de tecnologia na coleta e comercialização de dados pessoais dos usuários, principalmente para fins de marketing, tornou-se uma prática comum a ocultação de contratos de Termos de Serviço, camuflados por meio de links pequenos e escondidos que levam a textos muitas vezes em uma formatação pouco intuitiva, extremamente extensos e de linguagem densa, técnica e jurídica. Os contratos são omissos em favor de botões de confirmação, popularmente conhecidos como checkboxes contendo apenas o texto \"Li e aceito os termos de uso\", os quais muitas vezes induzem o usuário a aceitar o contrato sem lê-lo.\n",
    "\n",
    "Com isso, apresento neste trabalho um modelo de detecção de cláusulas potencialmente injustas em contratos de Termos de Serviço relativos utilizando um modelo de classificação de sequências treinado como um fine-tuning do LEGAL-BERT, uma arquitetura BERT pré-treinada em textos jurídicos dos Estados Unidos e da União Europeia. O fine-tuning e teste do modelo foi feito utilizando uma base de cerca de 20.000 cláusulas agrupadas em 10 classes: uma representando cláusulas justas e nove representando diferentes tipos de injustiças ao usuário que as cláusulas podem apresentar. Será apresentado o tratamento dos dados, o pré-processamento, a definição do modelo utilizado e dos hiperparâmetros de treinamento e os resultados gerais e por classe obtidos.\n",
    "\n",
    "Os pacotes utilizados neste trabalho são os seguintes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-12-13 22:25:53.981785: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from datasets import Dataset, DatasetDict\n",
    "\n",
    "from transformers import AutoTokenizer\n",
    "from transformers import DataCollatorWithPadding\n",
    "\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "from numpy import argmax\n",
    "\n",
    "import torch\n",
    "from transformers import AutoModelForSequenceClassification\n",
    "from transformers import Trainer, TrainingArguments\n",
    "\n",
    "from torch.utils.data import DataLoader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Trabalhos Relacionados\n",
    "\n",
    "As principais abrodagens de classificação de cláusulas de Termos de Serviço por grau de injustiça foram feitas por Lippe et. al nos próprios artigos que introduzem a base utilizada neste trabalho [[1](#ref)][[2](#ref)], nos quais foram testados os modelos considerados estado-da-arte na época: SVMs, CNNs, LSTMs em 2019 e, no artigo mais recente, End-to-End Memory Networks (MANN), uma arquitetura recorrente de 2015[[5](#ref)]. Até o momento, nenhum artigo da literatura tentou aplicar modelos baseados em transformer encoders como o BERT, que são o estado-da-arte atual em classificação de sentenças, a esta tarefa. O mais próximo já feito foi um classificador de cláusulas injustas no contexto de contratos comerciais e empresariais gerado por Singhal et. al por meio de fine-tuning do BERT original, atingindo acurácia de 84% [[3](#ref)]."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Base de Dados\n",
    "\n",
    "A base de dados utilizada foi a ToS100, uma base composta de cerca de 20.0000 cláusulas, na forma de strings, extraídas de 100 documentos dos Estados Unidos referentes às empresas de serviços de tecnologia mais relevantes em termos de relevância global, número de usuários e tempo desde que o serviço foi estabelecido. A base constitui a maior e mais completa fonte de cláusulas de Termos de Serviço, rotuladas ou não, disponível na literatura. Ela foi gerada por Lippe et. al em 2019 e atualizada em 2021, e se encontra disponível no github do artigo [[4](#ref)].\n",
    "\n",
    "A base classifica cada cláusula como uma de nove categorias de injustiça não-excludentes:\n",
    "\n",
    "* **CR - Remoção de Conteúdo:** A cláusula confere ao provedor o direito de modificar/excluir o conteúdo do usuário, incluindo compras dentro do aplicativo;\n",
    "\n",
    "* **LTD - Limitação de Responsabilidade:** A cláusula estipula que o dever de pagar danos é limitado ou excluído ao provedor do serviço sob determinadas condições;\n",
    "\n",
    "* **J - Jurisdição:** A cláusula especifica quais tribunais terão competência para julgar disputas sob o contrato. Neste caso, foram consideradas as cláusulas que estabelecem que qualquer processo judicial possa ocorrer fora do local de residência do usuário (ou seja, em uma cidade ou país diferente);\n",
    "\n",
    "* **LAW - Escolha de Lei:** A cláusula especifica qual lei regerá o contrato, significando também qual lei será aplicada na eventual resolução de uma disputa decorrente do contrato. Neste caso, foram consideradas as cláusulas que estabelecem que o contrato será regido por leis de um país potencialmente diferente do país de residência do usuário;\n",
    "\n",
    "* **A - Arbitragem:** A cláusula requer ou permite que as partes resolvam disputas por meio de um processo de arbitragem, antes que o caso vá para o tribunal, com algumas cláusulas permitindo que a decisão do árbitro do processo não necessariamente siga o que é definido na lei;\n",
    "\n",
    "* **CH - Mudança Unilateral:** A cláusula permite que o provedor de serviços altere e modifique os termos de serviço e/ou o próprio serviço sob determinadas condições;\n",
    "\n",
    "* **TER - Rescisão Unilateral:** A cláusula confere ao provedor o direito de suspender e/ou rescindir o serviço e/ou o contrato sob determinadas condições;\n",
    "\n",
    "* **PINC - Privacidade Inclusa:** A cláusula afirma que os consumidores consentem com a política de privacidade simplesmente usando o serviço;\n",
    "\n",
    "* **USE - Contrato por Uso:** A cláusula estipula que o consumidor está vinculado aos termos de uso do serviço simplesmente usando-o, sem a necessidade de marcar que leu e aceitou esses termos.\n",
    "\n",
    "É importante ressaltar que as labels classificam as cláusulas com relação à sua injustiça aparente em relação ao usuário/consumidor, o que pode ser uma definição subjetiva. É notável que a maioria dos tipos de cláusulas descritos são legais e, sob condições válidas, não apresentam injustiça ao usuário. Por isso, este modelo se destina a detectar cláusulas potencialmente injustas em um contrato, ou seja, que sejam relevantes o suficiente para que sejam lidas e interpretadas pelo usuário ao fim.\n",
    "\n",
    "O dataset original define, assim, o problema como um problema de classificação multi-label, com cada cláusula podendo possuir múltiplas labels dentre as descritas acima, e com as cláusulas justas sendo representadas por aquelas que não possuem nenhuma das labels. Observe uma visualização do dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>A</th>\n",
       "      <th>CH</th>\n",
       "      <th>CR</th>\n",
       "      <th>J</th>\n",
       "      <th>LAW</th>\n",
       "      <th>LTD</th>\n",
       "      <th>PINC</th>\n",
       "      <th>TER</th>\n",
       "      <th>USE</th>\n",
       "      <th>document</th>\n",
       "      <th>document_ID</th>\n",
       "      <th>label</th>\n",
       "      <th>text</th>\n",
       "      <th>TER_targets</th>\n",
       "      <th>LTD_targets</th>\n",
       "      <th>A_targets</th>\n",
       "      <th>CH_targets</th>\n",
       "      <th>CR_targets</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>Mozilla</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>websites &amp; communications terms of use</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>Mozilla</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>please read the terms of this entire document ...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>Mozilla</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>by accessing or signing up to receive communic...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>Mozilla</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>our websites include multiple domains such as ...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>Mozilla</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>you may also recognize our websites by nicknam...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0  A  CH  CR  J  LAW  LTD  PINC  TER  USE document  document_ID   \n",
       "0           0  0   0   0  0    0    0     0    0    0  Mozilla            0  \\\n",
       "1           1  0   0   0  0    0    0     0    0    0  Mozilla            0   \n",
       "2           2  0   0   0  0    0    0     0    0    1  Mozilla            0   \n",
       "3           3  0   0   0  0    0    0     0    0    0  Mozilla            0   \n",
       "4           4  0   0   0  0    0    0     0    0    0  Mozilla            0   \n",
       "\n",
       "   label                                               text TER_targets   \n",
       "0      0             websites & communications terms of use         NaN  \\\n",
       "1      0  please read the terms of this entire document ...         NaN   \n",
       "2      1  by accessing or signing up to receive communic...         NaN   \n",
       "3      0  our websites include multiple domains such as ...         NaN   \n",
       "4      0  you may also recognize our websites by nicknam...         NaN   \n",
       "\n",
       "  LTD_targets A_targets CH_targets CR_targets  \n",
       "0         NaN       NaN        NaN        NaN  \n",
       "1         NaN       NaN        NaN        NaN  \n",
       "2         NaN       NaN        NaN        NaN  \n",
       "3         NaN       NaN        NaN        NaN  \n",
       "4         NaN       NaN        NaN        NaN  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tos100_raw_df = pd.read_csv('ToS_100.csv')\n",
    "tos100_raw_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Após analisar o dataset, podemos perceber que, embora as cláusulas sejam apresentadas como um problema de classificação multilabel, apenas 1% das instâncias possuem mais de uma classe. Com isso, optei por descartar as cláusulas multilabel e transformar o problema em um problema de classificação multiclasse, com classes multuamente excludentes. Desse modo, cada cláusula injusta restante foi classificada com sua respectiva label, adicionando o prefixo \"UNFAIR-\" para facilitar a interpretação (por exemplo, uma cláusula com uma única label \"CR\" foi classificada na classe \"UNFAIR-CR\") e as cláusulas sem nenhuma label foram clássificadas como \"FAIR\" (justas):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cláusulas multilabel: 205\n",
      "Clausulas totais:   20417\n"
     ]
    }
   ],
   "source": [
    "labels_num = tos100_raw_df[['A','CH','CR','J','LAW','LTD','PINC','TER','USE']].sum(axis=1)\n",
    "print(f'Cláusulas multilabel: {len(labels_num[labels_num > 1])}')\n",
    "print(f'Clausulas totais:   {len(tos100_raw_df)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>company</th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Mozilla</td>\n",
       "      <td>websites &amp; communications terms of use</td>\n",
       "      <td>FAIR</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Mozilla</td>\n",
       "      <td>please read the terms of this entire document ...</td>\n",
       "      <td>FAIR</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Mozilla</td>\n",
       "      <td>by accessing or signing up to receive communic...</td>\n",
       "      <td>UNFAIR-USE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Mozilla</td>\n",
       "      <td>our websites include multiple domains such as ...</td>\n",
       "      <td>FAIR</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Mozilla</td>\n",
       "      <td>you may also recognize our websites by nicknam...</td>\n",
       "      <td>FAIR</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   company                                               text       label\n",
       "0  Mozilla             websites & communications terms of use        FAIR\n",
       "1  Mozilla  please read the terms of this entire document ...        FAIR\n",
       "2  Mozilla  by accessing or signing up to receive communic...  UNFAIR-USE\n",
       "3  Mozilla  our websites include multiple domains such as ...        FAIR\n",
       "4  Mozilla  you may also recognize our websites by nicknam...        FAIR"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Removemos as últimas colunas, sem informação relevante e com valores NaN\n",
    "tos100_df = tos100_raw_df.dropna(axis=1).copy()\n",
    "\n",
    "#Transformamos as colunas binárias de multilabels em uma coluna categória de labels.\n",
    "def get_row_label(row):\n",
    "    possible_labels = ['A','CH','CR','J','LAW','LTD','PINC','TER','USE']\n",
    "    labels = [label for label in possible_labels if row[label] == 1]\n",
    "\n",
    "    return 'FAIR' if len(labels) == 0 else f'UNFAIR-{labels[0]}' if len(labels) == 1 else 'MULTILABEL'\n",
    "\n",
    "tos100_df['label'] = tos100_df.apply(get_row_label, axis=1)\n",
    "\n",
    "#Removemos as entradas multilabel e selecionamos apenas a colunas necessárias (a empresa de origem, o texto de input e a label de output)\n",
    "\n",
    "tos100_df = tos100_df[tos100_df['label'] != 'MULTILABEL']\n",
    "tos100_df = tos100_df[['document', 'text', 'label']]\n",
    "tos100_df = tos100_df.rename(columns={'document':'company'}).reset_index(drop = True)\n",
    "\n",
    "tos100_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Divisão do Dataset em Treino, Validação e Teste\n",
    "\n",
    "O dataset foi dividido em treino, validação e teste em proporções 76%, 4% e 20%, respectivamente. Esta divisão foi gerada dividindo primeiro o dataset em treino e teste em proporções 80% e 20%, e em seguida dividindo o treino em treino e validação com proporções 95% e 5%. A divisão foi feita de maneira estratificada, uma vez que o dataset é extremamente desbalanceado em favor de cláusulas justas, conforme será abordado mais à frente."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6855cd2a821a419e8fad468685ccf176",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Casting to class labels:   0%|          | 0/20212 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{'label': 0,\n",
       " 'text': \"some particularly egregious examples of `` bad things '' are listed in this section .\"}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Geração do dataset huggingface e separação em treino, validação e teste\n",
    "\n",
    "raw_dataset = Dataset.from_pandas(tos100_df[['label', 'text']]).class_encode_column('label')\n",
    "trainval_test = raw_dataset.train_test_split(test_size=0.2, stratify_by_column='label')\n",
    "train_val = trainval_test['train'].train_test_split(test_size=0.05, stratify_by_column='label')\n",
    "\n",
    "tos100_dataset = DatasetDict({\n",
    "    'train': train_val['train'],\n",
    "    'validation': train_val['test'],\n",
    "    'test': trainval_test['test']\n",
    "})\n",
    "\n",
    "label_list = tos100_dataset['train'].features['label'].names\n",
    "id2label = {idx: label for idx, label in enumerate(label_list)}\n",
    "label2id = {label: idx for idx, label in enumerate(label_list)}\n",
    "\n",
    "tos100_dataset['train'][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modelo Utilizado e Pré-Processamento\n",
    "\n",
    "O modelo proposto neste trabalho se trata de um fine-tuning do LEGAL-BERT para a tarefa de classificação de sequências. O LEGAL-BERT é a arquitetura BERT base da família dos LEGAL-BERTS, modelos BERT pré-treinados em textos jurídicos [[6](#ref)]. O LEGAL-BERT-base, especificamente, possui um vocabulário próprio e foi pré-treinado com textos jurídicos legais dos EUA e da União Europeia, que constituem justamente a base contextual geral do dataset de cláusulas de Termos de Serviço utilizado. \n",
    "\n",
    "Como o LEGAL-BERT possui um vocabulário e tokenizer próprios, seu tokenizer foi utilizado neste trabalho para pré-processar as entradas:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1fb4a849073b458783ac2201f54ccb58",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/15360 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "85a7b6eefa7a485b84ed3b47df536567",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/809 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d845626fe70f44039667ed19f20b6f20",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/4043 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"nlpaueb/legal-bert-base-uncased\")\n",
    "\n",
    "def preprocess(examples):\n",
    "  return tokenizer(examples['text'], truncation=True)\n",
    "\n",
    "tokenized_dataset = tos100_dataset.map(preprocess, batched=True)\n",
    "data_collator = DataCollatorWithPadding(tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Métricas de Avaliação\n",
    "\n",
    "Para avaliar a validação e o teste, serão usadas as métricas básicas de classificação multiclasse: acurácia, precisão, revocação e f1. Como o dataset é extremamente desbalanceado em favor da classe de cláusulas justas, em relação às demais 8 classes de cláusulas injustas, a precisão, revocação e f1 serão calculadas tirando a média aritméticas entre as métricas de cada classe, como forma de tomar com a mesma importância todas as classes, mesmo que com tamanhos diferentes.\n",
    "\n",
    "Para a métrica de avaliação classe-a-classe do teste, será utilizada a revocação, uma vez que ela mede justamente o atributo de interesse do trabalho. Como o grau de injustiça de uma cláusula depende da interpretação pessoal do usuário, é possível que mesmo as cláusulas classificadas como potencialmente injustas não sejam interpretadas como injustas pelo usuário. Com isso, existe um grau de tolerância maior a cláusulas justas classificadas como injustas (falsos negativos).\n",
    "\n",
    "Por outro lado, não é interessante que o nosso modelo classifique como justa uma cláusula potencialmente injusta, já que este erro pode levar à aceitação de um contrato injusto por parte do usuário. Por isso, optamos pela métrica de recall, que mede justamente a porcentagem de cláusulas identificadas corretamente como pertencente à classe em relação ao total de cláusulas daquela classe. Ou seja, ao tentar maximizar o recall, desejamos que todas as cláusulas da classe sejam identificadas, mesmo que cláusulas de outras classes sejam identificadas incorretamente no processo, como o caso dos falsos-positivos para cláusulas justas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_metrics(p):\n",
    "  prediction_logits, true_labels = p\n",
    "  predicted_labels = argmax(prediction_logits, axis=1)\n",
    "\n",
    "  return {\n",
    "    \"precision\": precision_score(true_labels, predicted_labels, average = 'macro'),\n",
    "    \"recall\": recall_score(true_labels, predicted_labels, average = 'macro'),\n",
    "    \"f1\": f1_score(true_labels, predicted_labels, average = 'macro'),\n",
    "    \"accuracy\": accuracy_score(true_labels, predicted_labels),\n",
    "  }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hiperparâmetros do Modelo\n",
    "\n",
    "Conforme mencionado anteriormente, o modelo base utilizado é o LEGAL-BERT. Como forma de otimizar o espaço alocado na GPU, a versão com pesos do tipo float32 foi utilizada. Como parâmetros do modelo, estou usando taxa de aprendizado 0.00002, 3 épocas, weight decay de 0.1 e tamanho de batch igual a 3. O tamanho de batch foi escolhido devido às limitações de memória, e a taxa de aprendizado, número de épocas e weight decay foram escolhidos usando como referência os minicursos do huggingface utilizando modelos baseados no BERT.\n",
    "\n",
    "No código a seguir, temos a instanciação do modelo, definição dos hiperparâmetros e inicialização do treinador com os dados de treino e validação. Na célula seguinte, o modelo é treinado."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at nlpaueb/legal-bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.predictions.transform.dense.bias', 'cls.predictions.decoder.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.predictions.bias']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at nlpaueb/legal-bert-base-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "torch.cuda.empty_cache()\n",
    "\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\"nlpaueb/legal-bert-base-uncased\", torch_dtype=torch.float32, num_labels = len(label_list), id2label = id2label, label2id = label2id)\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"model_results\",\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=3,\n",
    "    per_device_eval_batch_size=3,\n",
    "    num_train_epochs=3,\n",
    "    weight_decay=0.01,\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    load_best_model_at_end=True,\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_dataset[\"train\"],\n",
    "    eval_dataset=tokenized_dataset[\"validation\"],\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=data_collator,\n",
    "    compute_metrics=compute_metrics,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/andre/miniconda3/envs/notebook_env/lib/python3.10/site-packages/transformers/optimization.py:407: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9ce10244a7f94780b59f7575111010d7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/15360 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.7061, 'learning_rate': 1.9348958333333336e-05, 'epoch': 0.1}\n",
      "{'loss': 0.5827, 'learning_rate': 1.869791666666667e-05, 'epoch': 0.2}\n",
      "{'loss': 0.4867, 'learning_rate': 1.8046875e-05, 'epoch': 0.29}\n",
      "{'loss': 0.3824, 'learning_rate': 1.7395833333333334e-05, 'epoch': 0.39}\n",
      "{'loss': 0.3228, 'learning_rate': 1.6744791666666668e-05, 'epoch': 0.49}\n",
      "{'loss': 0.3115, 'learning_rate': 1.609375e-05, 'epoch': 0.59}\n",
      "{'loss': 0.3084, 'learning_rate': 1.5442708333333335e-05, 'epoch': 0.68}\n",
      "{'loss': 0.3509, 'learning_rate': 1.479166666666667e-05, 'epoch': 0.78}\n",
      "{'loss': 0.289, 'learning_rate': 1.4140625000000002e-05, 'epoch': 0.88}\n",
      "{'loss': 0.2736, 'learning_rate': 1.3489583333333334e-05, 'epoch': 0.98}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e3145a0f817a47b8854ab08d13bcd3c7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/270 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/andre/.local/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.27394580841064453, 'eval_precision': 0.641146307458892, 'eval_recall': 0.6348391448733914, 'eval_f1': 0.6334418138400439, 'eval_accuracy': 0.9493201483312732, 'eval_runtime': 8.0994, 'eval_samples_per_second': 99.885, 'eval_steps_per_second': 33.336, 'epoch': 1.0}\n",
      "{'loss': 0.2158, 'learning_rate': 1.283854166666667e-05, 'epoch': 1.07}\n",
      "{'loss': 0.222, 'learning_rate': 1.2187500000000001e-05, 'epoch': 1.17}\n",
      "{'loss': 0.2172, 'learning_rate': 1.1536458333333334e-05, 'epoch': 1.27}\n",
      "{'loss': 0.2073, 'learning_rate': 1.0885416666666668e-05, 'epoch': 1.37}\n",
      "{'loss': 0.1648, 'learning_rate': 1.0234375000000001e-05, 'epoch': 1.46}\n",
      "{'loss': 0.2201, 'learning_rate': 9.583333333333335e-06, 'epoch': 1.56}\n",
      "{'loss': 0.1692, 'learning_rate': 8.932291666666668e-06, 'epoch': 1.66}\n",
      "{'loss': 0.2228, 'learning_rate': 8.281250000000001e-06, 'epoch': 1.76}\n",
      "{'loss': 0.2506, 'learning_rate': 7.630208333333334e-06, 'epoch': 1.86}\n",
      "{'loss': 0.2029, 'learning_rate': 6.979166666666667e-06, 'epoch': 1.95}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bc05faddf48b4e0d8de64a14a0c35070",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/270 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/andre/.local/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.2703166604042053, 'eval_precision': 0.5985592816274635, 'eval_recall': 0.6602729348277294, 'eval_f1': 0.6251847789891268, 'eval_accuracy': 0.9443757725587144, 'eval_runtime': 8.1081, 'eval_samples_per_second': 99.777, 'eval_steps_per_second': 33.3, 'epoch': 2.0}\n",
      "{'loss': 0.1467, 'learning_rate': 6.3281250000000005e-06, 'epoch': 2.05}\n",
      "{'loss': 0.1164, 'learning_rate': 5.677083333333334e-06, 'epoch': 2.15}\n",
      "{'loss': 0.1111, 'learning_rate': 5.026041666666667e-06, 'epoch': 2.25}\n",
      "{'loss': 0.1329, 'learning_rate': 4.3750000000000005e-06, 'epoch': 2.34}\n",
      "{'loss': 0.1242, 'learning_rate': 3.7239583333333335e-06, 'epoch': 2.44}\n",
      "{'loss': 0.1366, 'learning_rate': 3.072916666666667e-06, 'epoch': 2.54}\n",
      "{'loss': 0.0949, 'learning_rate': 2.421875e-06, 'epoch': 2.64}\n",
      "{'loss': 0.1576, 'learning_rate': 1.7708333333333337e-06, 'epoch': 2.73}\n",
      "{'loss': 0.1198, 'learning_rate': 1.1197916666666667e-06, 'epoch': 2.83}\n",
      "{'loss': 0.1338, 'learning_rate': 4.6875000000000006e-07, 'epoch': 2.93}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5cb5ee191b8a4d8da94d245ee15a379d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/270 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/andre/.local/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.30348947644233704, 'eval_precision': 0.656730519025601, 'eval_recall': 0.6552615193026152, 'eval_f1': 0.6521785576764337, 'eval_accuracy': 0.9517923362175525, 'eval_runtime': 8.1186, 'eval_samples_per_second': 99.648, 'eval_steps_per_second': 33.257, 'epoch': 3.0}\n",
      "{'train_runtime': 2022.1693, 'train_samples_per_second': 22.787, 'train_steps_per_second': 7.596, 'train_loss': 0.24181095696985722, 'epoch': 3.0}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "873c39c779054db1be34da357e84d441",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/270 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/andre/.local/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'eval_loss': 0.2703166604042053,\n",
       " 'eval_precision': 0.5985592816274635,\n",
       " 'eval_recall': 0.6602729348277294,\n",
       " 'eval_f1': 0.6251847789891268,\n",
       " 'eval_accuracy': 0.9443757725587144,\n",
       " 'eval_runtime': 8.098,\n",
       " 'eval_samples_per_second': 99.901,\n",
       " 'eval_steps_per_second': 33.341,\n",
       " 'epoch': 3.0}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()\n",
    "results = trainer.evaluate()\n",
    "results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inferência e Teste\n",
    "\n",
    "As métricas de validação do modelo acima sugerem resultados promissores, com acurácia de 95% e demais métricas de cerca de 70%. Vamos agora realizar a inferência e calcular as métricas de desempenho para o conjunto de teste.\n",
    "\n",
    "O código a seguir implementa as funções de inferência e teste, e testa o modelo implementado contra a base de teste que criamos a partir do ToS100:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/andre/miniconda3/envs/notebook_env/lib/python3.10/site-packages/transformers/trainer_pt_utils.py:411: FutureWarning: DistributedTensorGatherer is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "254ac372dad74e12ac1f32f3d83a65c3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/253 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Selecionamos o dataset de teste\n",
    "test_dataset = tokenized_dataset['test']\n",
    "\n",
    "#Selecionamos a coluna de labels verdadeiras e a removemos do dataset de inferência \n",
    "true_labels = test_dataset['label']\n",
    "\n",
    "test_dataset = test_dataset.remove_columns(['text', 'label'])\n",
    "\n",
    "#Realizamos a inferência em batches para todo o conjunto de testes\n",
    "test_loader = DataLoader(test_dataset, batch_size=16, collate_fn=data_collator)\n",
    "prediction_logits, _, _, _ = trainer.prediction_loop(test_loader, description='prediction')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Acurácia do teste:        0.9478110314123176\n",
      "Precisão média do teste:  0.6838669328804864\n",
      "Revocação média do teste: 0.7302754979077282\n",
      "F1 médio do teste:        0.7003685388022327\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/andre/.local/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "#Calculamos as métricas gerais do modelo para a base de teste\n",
    "predicted_labels = argmax(prediction_logits, axis=1)\n",
    "\n",
    "print(f'Acurácia do teste:        {accuracy_score(true_labels, predicted_labels)}')\n",
    "print(f'Precisão média do teste:  {precision_score(true_labels, predicted_labels, average=\"macro\")}')\n",
    "print(f'Revocação média do teste: {recall_score(true_labels, predicted_labels, average=\"macro\")}')\n",
    "print(f'F1 médio do teste:        {f1_score(true_labels, predicted_labels, average=\"macro\")}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Resultados Gerais\n",
    "\n",
    "Como podemos observar acima, o modelo apresentou, conforme observado na validação, desempenho de cerca de 95% para acurácia e cerca de 70% nas demais métricas. Embora 70% de precisão e principalmente recall não sejam os valores ideais no contexto de uma classificação, ainda podemos considerar os resultados como bons, principalmente levando em consideração o balanceamento da base. Conforme mencionado anteriormente, a base é extremamente desbalanceada em favor de cláusulas justas. Conforme poderá ser observado na tabela seguinte, as cláusulas justas constituem 89% de toda a base, com as demais 9 classes de cláusulas injustas constituindo os demais 11%, variando cada uma de 3% a 0.25% de todas as instâncias. Com isso, levando em consideração o desbalanceamento da base, podemos considerar o resultado como positivo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Revocações totais:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>recall</th>\n",
       "      <th>Nº of instances</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>UNFAIR-J</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>114</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>FAIR</th>\n",
       "      <td>0.968750</td>\n",
       "      <td>18239</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>UNFAIR-LAW</th>\n",
       "      <td>0.920000</td>\n",
       "      <td>124</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>UNFAIR-USE</th>\n",
       "      <td>0.875000</td>\n",
       "      <td>242</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>UNFAIR-TER</th>\n",
       "      <td>0.806452</td>\n",
       "      <td>308</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>UNFAIR-LTD</th>\n",
       "      <td>0.750000</td>\n",
       "      <td>599</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>UNFAIR-A</th>\n",
       "      <td>0.666667</td>\n",
       "      <td>105</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>UNFAIR-CH</th>\n",
       "      <td>0.660714</td>\n",
       "      <td>281</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>UNFAIR-CR</th>\n",
       "      <td>0.655172</td>\n",
       "      <td>146</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>UNFAIR-PINC</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>54</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               recall  Nº of instances\n",
       "UNFAIR-J     1.000000              114\n",
       "FAIR         0.968750            18239\n",
       "UNFAIR-LAW   0.920000              124\n",
       "UNFAIR-USE   0.875000              242\n",
       "UNFAIR-TER   0.806452              308\n",
       "UNFAIR-LTD   0.750000              599\n",
       "UNFAIR-A     0.666667              105\n",
       "UNFAIR-CH    0.660714              281\n",
       "UNFAIR-CR    0.655172              146\n",
       "UNFAIR-PINC  0.000000               54"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Calculamos o recall de cada classe e apresentamos, junto ao número total de intâncias da classe na base\n",
    "label_recalls = recall_score(true_labels, predicted_labels, labels=range(len(label_list)), average=None)\n",
    "\n",
    "s = pd.Series({label: precision for label, precision in zip(label_list, label_recalls)})\n",
    "df = pd.DataFrame(s, columns=['recall'])\n",
    "\n",
    "df['Nº of instances'] = tos100_df[['label', 'text']].groupby('label').count()\n",
    "\n",
    "sorted_df = df.sort_values('recall', ascending = False)\n",
    "print('Revocações totais:')\n",
    "display(sorted_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Resultados de Revocação por Classe\n",
    "\n",
    "Na tabela acima temos a revocação (recall) da classificação de cada classe, junto ao número de instâncias da classe na base completa.\n",
    "\n",
    "Conforme mencionado, as classes injustas possuem muito menos instâncias do que as classes justas. De maneira geral, a distribuição de instâncias afetou bastante os resultados de cada classe, com classes com menos instâncias apresentando recalls menores que classes com mais instâncias. Neste contexto, se destacam as cláusulas justas (\"FAIR\"), que são as mais proeminentes e apresentaram maior recall, e as de privacidade inclusa (\"PINC\"), que, sendo a menos presente nos dados, contendo apenas 54 instâncias (0.25%), não teve nenhum exemplo classificado corretamente, e com isso teve recall 0. Isso é especialmente preocupante, uma vez que, dentre todas as cláusulas, as cláusulas PINC foram as únicas discutidas no artigo da base de dados como claramente injustas, ao invés de potencialmente.\n",
    "\n",
    "No entanto, algumas classes apresentaram altas revocações apesar de não estarem entre as classes contendo a maior quantidade de exemplos, sendo elas a de jurisdição (\"J\"), com recall total (100%) e a de escolha de lei (\"LAW\"). Possivelmente, o que destacou esta classe foram termos relacionados a localidade, uma vez que os critérios de injustiça de ambas dizem respeito ao local (cidade, estado, país) da lei ou jurisdição levada em consideração, e são as únicas que os têm como temas principais."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusão\n",
    "\n",
    "A partir dos resultados obtidos, podemos concluir que, embora o modelo tenha apresentado resultados promissores, ainda podemos explorar mais formas de melhorar o seu desempenho, principalmente levando em consideração o potencial que o modelo apresenta para a tarefa de classificação de sequências em outros âmbitos. No entanto, os resultados foram majoritariamente positivos, com 3 das classes classificadas apresentando desempenho (recall) acima de 90%, 7 apresentando recall acima de 70% e 9 apresentando recall acima de 65%. Um dos principais desafios encontrados na tarefa é a definição de uma base consistente e balanceada, uma vez que existem poucas bases para a tarefa explorada e todas apresentam um desbalanceamento grande na base com relação a cláusulas justas. Por fim, com este trabalho pudemos aplicar modelos estado-da-arte em processamento de linguagem natural a tarefas relevantes para o dia-a-dia de pessoas inseridas no contexto de serviços tecnológicos com resultados consideravelmetne positivos em relação ao que já foi feito até o momento neste contexto."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Referências<a id='ref'></a>\n",
    "\n",
    "1. Ruggeri, F., Lagioia, F., Lippi, M. et al. (2022) [Detecting and explaining unfairness in consumer contracts through memory networks.](https://doi.org/10.1007/s10506-021-09288-2) Artif Intell Law 30, 59–92.\n",
    "\n",
    "\n",
    "2. Lippi M, Pałka P, Contissa G, Lagioia F, Micklitz HW, Sartor G, Torroni P (2019) [CLAUDETTE: an automated detector of potentially unfair clauses in online terms of service.](https://arxiv.org/abs/1805.01217) Artif Intell Law 27(2):117–139\n",
    "\n",
    "3. Ruggeri, F., Lagioia, F., Lippi, M. et al. (2022) [Memnet_ToS Repository](https://github.com/federicoruggeri/Memnet_ToS/tree/master) Github.\n",
    "\n",
    "4. Singhal et al., (2023). [Towards Mitigating Perceived Unfairness in Contracts from a Non-Legal Stakeholder’s Perspective](https://aclanthology.org/2023.nllp-1.11) NLLP-WS 2023\n",
    "\n",
    "5. Sukhbaatar S, Weston J, Fergus R, et al (2015) [End-to-end memory networks. In: Advances in neural information processing systems](https://arxiv.org/abs/1503.08895), pp 2440–2448\n",
    "\n",
    "6. I. Chalkidis, M. Fergadiotis, P. Malakasiotis, N. Aletras and I. Androutsopoulos. (2020)[\"LEGAL-BERT: The Muppets straight out of Law School\". In Findings of Empirical Methods in Natural Language Processing](https://aclanthology.org/2020.findings-emnlp.261) EMNLP 2020."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "notebook_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
